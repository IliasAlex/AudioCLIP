{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilias/miniconda3/envs/adversarial_thesis/lib/python3.9/site-packages/ignite/handlers/checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "import tqdm\n",
    "import signal\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "import torchvision as tv\n",
    "\n",
    "import ignite.engine as ieng\n",
    "import ignite.metrics as imet\n",
    "import ignite.handlers as ihan\n",
    "\n",
    "from typing import Any\n",
    "from typing import Dict\n",
    "from typing import List\n",
    "from typing import Type\n",
    "from typing import Union\n",
    "from typing import Optional\n",
    "\n",
    "from termcolor import colored\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections.abc import Iterable\n",
    "\n",
    "from ignite_trainer import _utils\n",
    "from ignite_trainer import _visdom\n",
    "from ignite_trainer import _interfaces\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pretrained_model = '/home/ilias/projects/AudioCLIP/assets/UrbanSound8K_Multimodal-Audio-x2_ACLIP-CV01/UrbanSound8K_Multimodal-Audio-x2_ACLIP-CV01_ACLIP-CV01_performance=0.9188.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    ToTensor1D()\n",
      "    RandomPadding()\n",
      "    RandomCrop()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "Dataset: Type = _utils.load_class('utils.datasets.UrbanSound8K')\n",
    "dataset_args = {\n",
    "    \"root\": \"/data/urbansound8k\",\n",
    "    \"sample_rate\": 44100, \n",
    "    \"fold\": 10, \n",
    "    \"mono\": False,\n",
    "    \"training\": {\"key\": \"train\", \"yes\": True, \"no\": False}\n",
    "}\n",
    "batch_train = 32\n",
    "batch_test = 32\n",
    "workers_train = 4\n",
    "workers_test = 4\n",
    "\n",
    "transforms = [\n",
    "    {\n",
    "      \"class\": \"utils.transforms.ToTensor1D\",\n",
    "      \"args\": {}\n",
    "    },\n",
    "    {\n",
    "      \"class\": \"utils.transforms.RandomFlip\",\n",
    "      \"args\": {\"p\": 0.5},\n",
    "      \"test\": False\n",
    "    },\n",
    "    {\n",
    "      \"class\": \"utils.transforms.RandomScale\",\n",
    "      \"args\": {\"max_scale\": 1.50},\n",
    "      \"test\": False\n",
    "    },\n",
    "    {\n",
    "      \"class\": \"utils.transforms.RandomPadding\",\n",
    "      \"args\": {\"out_len\": 176400},\n",
    "      \"test\": False\n",
    "    },\n",
    "    {\n",
    "      \"class\": \"utils.transforms.RandomCrop\",\n",
    "      \"args\": {\"out_len\": 176400},\n",
    "      \"test\": False\n",
    "    },\n",
    "    {\n",
    "      \"class\": \"utils.transforms.RandomNoise\",\n",
    "      \"args\": {\"snr_min_db\": 10.0, \"snr_max_db\": 120.0, \"p\": 0.25},\n",
    "      \"test\": False\n",
    "    },\n",
    "    {\n",
    "      \"class\": \"utils.transforms.RandomPadding\",\n",
    "      \"args\": {\"out_len\": 176400, \"train\": False},\n",
    "      \"train\": False\n",
    "    },\n",
    "    {\n",
    "      \"class\": \"utils.transforms.RandomCrop\",\n",
    "      \"args\": {\"out_len\": 176400, \"train\": False},\n",
    "      \"train\": False\n",
    "    }\n",
    "  ]\n",
    "\n",
    "transforms_train = list()\n",
    "transforms_test = list()\n",
    "\n",
    "for idx, transform in enumerate(transforms):\n",
    "    use_train = transform.get('train', True)\n",
    "    use_test = transform.get('test', True)\n",
    "\n",
    "    transform = _utils.load_class(transform['class'])(**transform['args'])\n",
    "\n",
    "    if use_train:\n",
    "        transforms_train.append(transform)\n",
    "    if use_test:\n",
    "        transforms_test.append(transform)\n",
    "\n",
    "    transforms[idx]['train'] = use_train\n",
    "    transforms[idx]['test'] = use_test\n",
    "\n",
    "transforms_train = tv.transforms.Compose(transforms_train)\n",
    "transforms_test = tv.transforms.Compose(transforms_test)\n",
    "print(transforms_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT RANDOM FOLD\n",
      "Loading UrbanSound8K (train=True)\n",
      "NOT RANDOM FOLD\n",
      "Loading UrbanSound8K (train=False)\n"
     ]
    }
   ],
   "source": [
    "train_loader, eval_loader = _utils.get_data_loaders(\n",
    "    Dataset,\n",
    "    dataset_args,\n",
    "    batch_train,\n",
    "    batch_test,\n",
    "    workers_train,\n",
    "    workers_test,\n",
    "    transforms_train,\n",
    "    transforms_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.audioclip import AudioCLIP\n",
    "\n",
    "model = AudioCLIP(pretrained=pretrained_model,\n",
    "                  multilabel=False)\n",
    "model = model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable all parameters\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# enable only audio-related parameters\n",
    "for p in model.audio.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "model.logit_scale_ai.requires_grad = True\n",
    "model.logit_scale_at.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5, momentum=0.9, nesterov=True, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 134.11M\n",
      "Number of trainable parameters: 134.11M\n"
     ]
    }
   ],
   "source": [
    "num_params_total = sum(p.numel() for p in model.parameters())\n",
    "num_params_train = sum(p.numel() for grp in optimizer.param_groups for p in grp['params'])\n",
    "\n",
    "params_total_label = ''\n",
    "params_train_label = ''\n",
    "if num_params_total > 1e6:\n",
    "    num_params_total /= 1e6\n",
    "    params_total_label = 'M'\n",
    "elif num_params_total > 1e3:\n",
    "    num_params_total /= 1e3\n",
    "    params_total_label = 'k'\n",
    "\n",
    "if num_params_train > 1e6:\n",
    "    num_params_train /= 1e6\n",
    "    params_train_label = 'M'\n",
    "elif num_params_train > 1e3:\n",
    "    num_params_train /= 1e3\n",
    "    params_train_label = 'k'\n",
    "tqdm.write('Total number of parameters: {:.2f}{}'.format(num_params_total, params_total_label))\n",
    "tqdm.write('Number of trainable parameters: {:.2f}{}'.format(num_params_train, params_train_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Ensure y_true and y_pred are lists\n",
    "y_true = []\n",
    "y_pred_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(eval_loader, desc=\"Evaluating\"):\n",
    "        audio, _, text = batch\n",
    "\n",
    "        ((audio_features, _, _), _), _ = model(\n",
    "            audio=audio,\n",
    "            batch_indices=torch.arange(audio.shape[0], dtype=torch.int64, device=device)\n",
    "        )\n",
    "        audio_features = audio_features.unsqueeze(1)\n",
    "\n",
    "        ((_, _, text_features), _), _ = model(\n",
    "            text=[\n",
    "                [eval_loader.dataset.class_idx_to_label[class_idx]]\n",
    "                for class_idx in sorted(eval_loader.dataset.class_idx_to_label.keys())\n",
    "            ],\n",
    "            batch_indices=torch.arange(\n",
    "                len(eval_loader.dataset.class_idx_to_label), dtype=torch.int64, device=device\n",
    "            )\n",
    "        )\n",
    "        text_features = text_features.unsqueeze(1).transpose(0, 1)\n",
    "\n",
    "        logit_scale_at = torch.clamp(model.logit_scale_at.exp(), min=1.0, max=100.0)\n",
    "        logits = (logit_scale_at * audio_features @ text_features.transpose(-1, -2)).squeeze(1)\n",
    "\n",
    "        y = torch.zeros(\n",
    "            audio.shape[0], len(eval_loader.dataset.class_idx_to_label), dtype=torch.int8, device=device\n",
    "        )\n",
    "        for item_idx, labels in enumerate(text):\n",
    "            class_ids = list(sorted([\n",
    "                eval_loader.dataset.label_to_class_idx[lb] for lb in labels\n",
    "            ]))\n",
    "            y[item_idx][class_ids] = 1\n",
    "\n",
    "        y_pred_batch = torch.softmax(logits, dim=-1)\n",
    "        y_pred_batch = y_pred_batch.argmax(dim=-1)  # Get predicted class index\n",
    "        y = y.argmax(dim=-1)  # Convert true labels to class indices for comparison\n",
    "\n",
    "        y_true.extend(y.cpu().numpy())\n",
    "        y_pred_list.extend(y_pred_batch.cpu().numpy())\n",
    "\n",
    "# Compute metrics\n",
    "y_true_np = np.array(y_true)\n",
    "y_pred_np = np.array(y_pred_list)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true_np, y_pred_np)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true_np, y_pred_np, target_names=list(eval_loader.dataset.class_idx_to_label.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [0.19977906346321106, 0.06820614635944366, 0.010029755532741547, 0.3603510558605194, 0.03957076370716095, 0.07147389650344849, 0.1348404586315155, 0.010213349014520645, 0.09584540128707886, 0.009690063074231148], 'predicted_label': 'dog bark'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "\n",
    "def pad_audio(waveform, target_length):\n",
    "    \"\"\"\n",
    "    Pads or truncates the waveform to the target length.\n",
    "\n",
    "    Args:\n",
    "        waveform (torch.Tensor): The audio waveform (C, L), where C is the channel and L is the length.\n",
    "        target_length (int): The target length in samples.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The padded or truncated waveform.\n",
    "    \"\"\"\n",
    "    current_length = waveform.shape[-1]\n",
    "\n",
    "    if current_length < target_length:\n",
    "        # Pad waveform to the right to reach the target length\n",
    "        padding = target_length - current_length\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "    elif current_length > target_length:\n",
    "        # Truncate waveform to the target length\n",
    "        waveform = waveform[:, :target_length]\n",
    "\n",
    "    return waveform\n",
    "\n",
    "def infer_audio_class(audio_path, model, device, label_mapping, transform_audio=None):\n",
    "    model.eval()\n",
    "\n",
    "    # Load and preprocess audio\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    waveform = pad_audio(waveform, sample_rate*4)\n",
    "    waveform = waveform.numpy()  # Ensure compatibility with `transforms_test``\n",
    "   \n",
    "    for t in transforms_test.transforms:\n",
    "        waveform = t(waveform)\n",
    "\n",
    "    # Move to GPU and free memory\n",
    "    torch.cuda.empty_cache()\n",
    "    waveform = waveform.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Model inference\n",
    "        ((audio_features, _, _), _), _ = model(\n",
    "            audio=waveform,\n",
    "            batch_indices=torch.arange(waveform.shape[0], dtype=torch.int64, device=device)\n",
    "        )\n",
    "        audio_features = audio_features.unsqueeze(1)\n",
    "\n",
    "        # Generate text features\n",
    "        ((_, _, text_features), _), _ = model(\n",
    "            text=[\n",
    "                [label_mapping[class_idx]]\n",
    "                for class_idx in sorted(label_mapping.keys())\n",
    "            ],\n",
    "            batch_indices=torch.arange(len(label_mapping), dtype=torch.int64, device=device)\n",
    "        )\n",
    "        text_features = text_features.unsqueeze(1).transpose(0, 1)\n",
    "\n",
    "        # Compute similarity scores\n",
    "        logit_scale_at = torch.clamp(model.logit_scale_at.exp(), min=1.0, max=100.0)\n",
    "        y_pred = (logit_scale_at * audio_features @ text_features.transpose(-1, -2)).squeeze(1)\n",
    "        y = torch.zeros(\n",
    "                    waveform.shape[0], len(eval_loader.dataset.class_idx_to_label), dtype=torch.int8, device=device\n",
    "                )\n",
    "           \n",
    "        predictions = torch.softmax(y_pred, dim=-1)\n",
    "        y = y.argmax(dim=-1)\n",
    "\n",
    "                \n",
    "        predictions = predictions.cpu().numpy().squeeze()\n",
    "        predicted_idx = np.argmax(predictions)\n",
    "        predicted_label = label_mapping[predicted_idx]\n",
    "\n",
    "    return {\n",
    "         \"predictions\": predictions.tolist(),\n",
    "         \"predicted_label\": predicted_label\n",
    "     }\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "# Assuming label_mapping is a dictionary {0: \"class1\", 1: \"class2\", ...}\n",
    "audio_path = \"/home/ilias/projects/adversarial_thesis/data/orig_1_siren.wav\"\n",
    "result = infer_audio_class(audio_path, model, device, eval_loader.dataset.class_idx_to_label, transform_audio=transforms_test)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adversarial_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
